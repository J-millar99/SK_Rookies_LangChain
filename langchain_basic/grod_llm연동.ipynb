{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60909fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ff9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"자바는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bbbba",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c039ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\" \\\n",
    "\"You are an expert in AI Expert.\" \\\n",
    "\"Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b2542",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM + outputparser를 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8968c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe같은 products가 있어.\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5547e7",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해주세요.\"})\n",
    "    # 스트리밍 출력\n",
    "    print(answer)\n",
    "\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0bfea",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫 번째 Chain의 출력이, 두 번째 Chain의 입력이 된다.\n",
    "* 두 개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 3문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc772051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a15574",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da505839",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3ffe2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 요약해서 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 요약해서 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 요약해서 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69300835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='GPT-4 모델은 대규모 데이터셋을 기반으로 하는 딥러닝 알고리즘을 사용하여 학습되며, 자연어 처리 작업을 위해 특별히 설계된 트랜스포머 아키텍처를 활용합니다. 이 모델은 주어진 문맥을 이해하고 적절한 응답을 생성하도록 학습되며, 강화 학습 및 지도 학습을 통해 지속적으로 개선됩니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 30, 'total_tokens': 103, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.256568951, 'prompt_time': 0.003246002, 'completion_time': 0.154141496, 'total_time': 0.157387498}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'id': 'chatcmpl-5691ae20-c3cc-4b2f-8e2b-d7852b11d169', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--a2a2b8a2-bca7-4281-aa2a-20af43721128-0' usage_metadata={'input_tokens': 30, 'output_tokens': 73, 'total_tokens': 103, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='Gemma는 컴퓨터가 자연어를 이해하고 생성할 수 있도록 설계된 언어 모델입니다. Gemma는 대규모의 텍스트 데이터를 학습하여 언어의 패턴과 구조를 파악하고, 이를 기반으로 새로운 텍스트를 생성하거나 질문에 답변할 수 있습니다. Gemma는 트랜스포머 아키텍처를 기반으로 하며, 셀프 어텐션 메커니즘을 통해 입력 텍스트의 맥락을 이해하고, 더 정확한 예측을 생성합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 29, 'total_tokens': 124, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.212791907, 'prompt_time': 0.002655041, 'completion_time': 0.210894579, 'total_time': 0.21354962}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'id': 'chatcmpl-fe71586c-b060-4288-8a9b-172eada9acad', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--e6c9d7f1-d22c-4730-9ff8-8def4cfe9c68-0' usage_metadata={'input_tokens': 29, 'output_tokens': 95, 'total_tokens': 124, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='llama-4 모델은 메타에서 개발한 대규모 언어 모델입니다. 이 모델은 방대한 텍스트 데이터를 바탕으로 학습되며, 이를 통해 자연어 처리 능력을 습득합니다. 학습 과정에서 모델은 입력된 텍스트의 패턴과 구조를 분석하고, 이를 기반으로 다음에 나타날 단어 또는 문장을 예측합니다. 이러한 예측과정을 반복하면서 모델은 언어에 대한 이해와 생성 능력을 향상시키게 됩니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 31, 'total_tokens': 116, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.22886975599999998, 'prompt_time': 0.002654651, 'completion_time': 0.168921518, 'total_time': 0.171576169}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'id': 'chatcmpl-7459d498-be5e-4c7e-b1af-3e6b6a6b1e25', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--73e4ff56-2789-4f33-bc9a-598f2173b39f-0' usage_metadata={'input_tokens': 31, 'output_tokens': 85, 'total_tokens': 116, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) # AI Message 타입\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35fd96",
   "metadata": {},
   "source": [
    "### ChatMessagePromptTemplate\n",
    "* SystemMessagePrompt, HumanMessagePromptTemplate, AIMessagePromptTemplate를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a411048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "딥러닝은 인공신경망을 기반으로 하는 머신러닝의 한 분야입니다. 인공신경망은 인간의 뇌를 모방한 구조로, 데이터를 처리하고 학습하는 방식입니다. 딥러닝은 이러한 인공신경망을 깊게 쌓아서 복잡한 데이터들을 처리하고 학습하는 기술입니다.\n",
      "\n",
      "딥러닝은 데이터에서 자동으로 특징을 추출하고, 이를 통해 데이터를 분류하거나 예측하는 작업을 수행합니다. 이를 위해 딥러닝 모델은 대량의 데이터를 필요로 하며, 이를 통해 모델의 성능을 높일 수 있습니다.\n",
      "\n",
      "딥러닝의 핵심 개념은 다음과 같습니다.\n",
      "\n",
      "1. 인공신경망: 딥러닝은 인공신경망을 기반으로 합니다. 인공신경망은 입력층, 은닉층, 출력층으로 구성되며, 각 층은 여러 개의 노드로 구성됩니다.\n",
      "2. 깊이: 딥러닝은 인공신경망을 깊게 쌓아서 복잡한 데이터들을 처리합니다. 이는 데이터에서 자동으로 특징을 추출하고, 이를 통해 데이터를 분류하거나 예측하는 작업을 수행합니다.\n",
      "3. 학습: 딛러닝은 대량의 데이터를 필요로 하며, 이를 통해 모델의 성능을 높일 수 있습니다. 모델은 데이터를 학습하면서 자동으로 특징을 추출하고, 이를 통해 데이터를 분류하거나 예측하는 작업을 수행합니다.\n",
      "\n",
      "딥러닝은 다양한 분야에서 활용되고 있습니다. 예를 들어, 이미지 인식, 음성 인식, 자연어 처리, 추천 시스템 등이 있습니다. 또한, 딥러닝은 자율 주행 자동차, 의료 진단, 금융 예측 등 다양한 분야에서 활용되고 있습니다.\n",
      "\n",
      "딥러닝의 장점은 다음과 같습니다.\n",
      "\n",
      "* 복잡한 데이터 처리: 딥러닝은 복잡한 데이터를 처리할 수 있습니다.\n",
      "* 자동화된 특징 추출: 딥러닝은 데이터에서 자동으로 특징을 추출할 수 있습니다.\n",
      "* 높은 성능: 딥러닝은 대량의 데이터를 학습하면서 높은 성능을 발휘할 수 있습니다.\n",
      "\n",
      "하지만, 딥러닝의 단점은 다음과 같습니다.\n",
      "\n",
      "* 대량의 데이터 필요: 딥러닝은 대량의 데이터를 필요로 합니다.\n",
      "* 높은 계산 비용: 딥러닝은 높은 계산 비용이 필요합니다.\n",
      "* 해석 어려움: 딥러닝 모델의 결과를 해석하기 어려울 수 있습니다.\n",
      "\n",
      "결론적으로, 딥러닝은 인공신경망을 기반으로 하는 머신러닝의 한 분야로, 복잡한 데이터를 처리하고 학습하는 기술입니다. 딥러닝은 다양한 분야에서 활용되고 있으며, 높은 성능을 발휘할 수 있습니다. 하지만, 대량의 데이터를 필요로 하며, 높은 계산 비용이 필요하고, 해석이 어려울 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가입니다. 명확하고 자세하게 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"{topic}에 대한 예시 답입니다.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요??\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f270c1a",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 예시를 제공하는 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98329813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "1. **수성**: 태양과 가장 가까운 행성으로, 매우 작고 온도가 극심하게 변합니다.\n",
      "2. **금성**: 밝고 뜨거운 행성으로, 강한 온실 효과로 표면 온도가 매우 높습니다.\n",
      "3. **지구**: 생명체가 사는 유일한 행성으로, 물과 대기가 있습니다.\n",
      "4. **화성**: 붉은 행성으로, 과거에는 물이 있었을 것으로 추정됩니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성으로, 가스 거인입니다.\n",
      "6. **토성**: 아름다운 고리를 가진 가스 거인입니다.\n",
      "7. **천왕성**: 자전축이 기울어져 있어 극단적인 계절 변화를 겪습니다.\n",
      "8. **해왕성**: 태양계에서 가장 먼 행성으로, 강한 바람이 불어오는 곳입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "model = llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
